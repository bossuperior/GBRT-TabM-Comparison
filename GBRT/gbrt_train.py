import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from gbrt_model import FlexibleMLP
from pathlib import Path

# ==========================================
# 1. ‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å GBRT
# ==========================================
BEST_LAYERS = 2
BEST_NEURONS = 35
BEST_LR = 0.008535324065804302
BEST_DROPOUT = 0.03718515424302466
BEST_WEIGHT_DECAY = 1.7294309366607873e-05
BEST_BATCH_SIZE = 64

MAX_EPOCHS = 200  # ‡πÄ‡∏ó‡∏£‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 200 ‡∏£‡∏≠‡∏ö
PATIENCE = 20  # ‡∏ñ‡πâ‡∏≤ Val Loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á 20 ‡∏£‡∏≠‡∏ö‡∏ï‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ó‡∏£‡∏ô (Early Stopping)
# ==========================================

# 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Train, Val, Test)
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data" / "california"

X_train = torch.tensor(np.load(DATA_DIR / "X_num_train.npy")).float()
y_train = torch.tensor(np.load(DATA_DIR / "Y_train.npy")).float().view(-1, 1)

X_val = torch.tensor(np.load(DATA_DIR / "X_num_val.npy")).float()
y_val = torch.tensor(np.load(DATA_DIR / "Y_val.npy")).float().view(-1, 1)

# ** ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ï‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á (Test Set) ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ **
X_test = torch.tensor(np.load(DATA_DIR / "X_num_test.npy")).float()
y_test_np = np.load(DATA_DIR / "Y_test.npy")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Batching
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=BEST_BATCH_SIZE, shuffle=True)

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡∏ß‡∏à‡∏£‡∏¥‡∏á
model = FlexibleMLP(n_layers=BEST_LAYERS, n_neurons=BEST_NEURONS, dropout_rate=BEST_DROPOUT)
optimizer = torch.optim.Adam(model.parameters(), lr=BEST_LR, weight_decay=BEST_WEIGHT_DECAY)
criterion = nn.MSELoss()

best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = BASE_DIR / "GBRT" / "mlp_gbrt_model.pt"

print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô Final Model ‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î...")
print(f"Layers: {BEST_LAYERS}, Neurons: {BEST_NEURONS}, Batch Size: {BEST_BATCH_SIZE}")

# 4. Training Loop ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏∞‡∏ö‡∏ö
for epoch in range(MAX_EPOCHS):
    model.train()
    train_loss_accum = 0.0

    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡∏•‡∏∞ Batch
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        train_loss_accum += loss.item() * batch_X.size(0)

    avg_train_loss = train_loss_accum / len(train_dataset)

    # ‡∏ß‡∏±‡∏î‡∏ú‡∏• Validation ‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_loss = criterion(val_outputs, y_val).item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1:3d}/{MAX_EPOCHS}] | Train MSE: {avg_train_loss:.4f} | Val MSE: {val_loss:.4f}")

    # ‡∏£‡∏∞‡∏ö‡∏ö Early Stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), best_model_path)  # ‡πÄ‡∏ã‡∏ü‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= PATIENCE:
        print(f"\nüõë Early stopping ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏ö {epoch + 1}!")
        print(f"‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á...")
        break

# ==========================================
# 5. ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Test Evaluation)
# ==========================================
model.load_state_dict(torch.load(best_model_path))  # ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà Val Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
model.eval()

with torch.no_grad():
    # ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö Test Set ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï
    test_preds = model(X_test).numpy()

    final_test_rmse = np.sqrt(mean_squared_error(y_test_np, test_preds))
    final_test_r2 = r2_score(y_test_np, test_preds)

print("\n=========================================")
print(f"üèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ù‡∏±‡πà‡∏á MLP (‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö TEST SET)")
print(f"RMSE: {final_test_rmse:.4f} (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
print(f"R¬≤ Score: {final_test_r2:.4f} (‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 1.0 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
print("=========================================")

# ‡πÄ‡∏ã‡∏ü‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå txt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏∏‡πà‡∏° Reporting ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠
results_file = BASE_DIR / "GBRT" / "mlp_gbrt_results.txt"
with open(results_file, "w", encoding="utf-8") as f:
    f.write(f"Model: MLP tuned by GBRT\n")
    f.write(f"Test RMSE: {final_test_rmse:.4f}\n")
    f.write(f"Test R2: {final_test_r2:.4f}\n")
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {results_file} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")