import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from gbrt_model import FlexibleMLP

# ==========================================
# 1. ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å gbrt_tuner.py ‡∏•‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
# ==========================================
BEST_LAYERS = 3  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ GBRT ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 3
BEST_NEURONS = 128  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ GBRT ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 128
BEST_LR = 0.001  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ GBRT ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 0.001
MAX_EPOCHS = 200  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô
PATIENCE = 20  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ Val Loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î (Early Stopping)
# ==========================================

# 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡∏ä‡∏∏‡∏î (Train, Val, Test)
data_dir = "../data/california"  # ‡∏õ‡∏£‡∏±‡∏ö Path ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data
X_train = torch.tensor(np.load(f"{data_dir}/X_num_train.npy")).float()
y_train = torch.tensor(np.load(f"{data_dir}/Y_train.npy")).float().view(-1, 1)

X_val = torch.tensor(np.load(f"{data_dir}/X_num_val.npy")).float()
y_val = torch.tensor(np.load(f"{data_dir}/Y_val.npy")).float().view(-1, 1)

# **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å** ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Test Set ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö TabM
X_test = torch.tensor(np.load(f"{data_dir}/X_num_test.npy")).float()
y_test_np = np.load(f"{data_dir}/Y_test.npy")  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô numpy ‡πÑ‡∏ß‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≠‡∏ô‡∏à‡∏ö

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠
model = FlexibleMLP(n_layers=BEST_LAYERS, n_neurons=BEST_NEURONS)
optimizer = torch.optim.Adam(model.parameters(), lr=BEST_LR)
criterion = nn.MSELoss()

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Early Stopping
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = "best_gbrt_mlp.pt"

print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• MLP ‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î...")
print(f"Layers: {BEST_LAYERS}, Neurons: {BEST_NEURONS}, LR: {BEST_LR}")

# 4. Training Loop ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
for epoch in range(MAX_EPOCHS):
    # --- Train Mode ---
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    # --- Validation Mode ---
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_loss = criterion(val_outputs, y_val)

    # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÜ 10 Epoch
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1}/{MAX_EPOCHS}] | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}")

    # --- Early Stopping Logic ---
    if val_loss.item() < best_val_loss:
        best_val_loss = val_loss.item()
        epochs_no_improve = 0
        # ‡πÄ‡∏ã‡∏ü‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
        torch.save(model.state_dict(), best_model_path)
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= PATIENCE:
        print(f"üõë Early stopping ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏ö {epoch + 1}! ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤...")
        break

# 5. ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏±‡∏ö Test Set
model.load_state_dict(torch.load(best_model_path))
model.eval()

with torch.no_grad():
    # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Test
    test_preds = model(X_test).numpy()

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Metrics ‡πÄ‡∏ä‡∏¥‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
    final_rmse = np.sqrt(mean_squared_error(y_test_np, test_preds))
    final_r2 = r2_score(y_test_np, test_preds)

print("\n=========================================")
print(f"üèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ù‡∏±‡πà‡∏á MLP (‡∏ö‡∏ô Test Set)")
print(f"RMSE: {final_rmse:.4f} (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
print(f"R¬≤ Score: {final_r2:.4f} (‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 1.0 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
print("=========================================")

# (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏° Reporting
with open("gbrt_results.txt", "w") as f:
    f.write(f"RMSE,{final_rmse:.4f}\n")
    f.write(f"R2,{final_r2:.4f}\n")