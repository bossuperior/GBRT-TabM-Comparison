from skopt import forest_minimize
from skopt.space import Real, Integer, Categorical
from skopt.callbacks import EarlyStopper
import numpy as np
from sklearn.metrics import mean_squared_error
from gbrt_model import FlexibleMLP
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

#à¹ƒà¸ªà¹ˆà¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¸ˆà¸³à¸™à¸§à¸™à¸£à¸­à¸šà¹à¸¥à¸° RMSE à¹ƒà¸™à¸à¸²à¸£à¸ˆà¸¹à¸™
MAX_CALLS = 50
TARGET_RMSE = 0.65

BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data" / "california"

X_train = np.load(DATA_DIR / "X_num_train.npy")
y_train = np.load(DATA_DIR / "Y_train.npy")
X_val = np.load(DATA_DIR / "X_num_val.npy")
y_val = np.load(DATA_DIR / "Y_val.npy")

# --- à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™ Tensor à¹€à¸­à¸²à¹„à¸§à¹‰à¸¥à¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸² ---
X_train_t = torch.tensor(X_train).float()
y_train_t = torch.tensor(y_train).float().view(-1, 1)
X_val_t = torch.tensor(X_val).float()

# ==========================================
# 1. à¸ªà¸£à¹‰à¸²à¸‡ Dataset à¹€à¸•à¸£à¸µà¸¢à¸¡à¹„à¸§à¹‰à¹ƒà¸«à¹‰ DataLoader
# ==========================================
train_dataset = TensorDataset(X_train_t, y_train_t)

iteration_count = 0


def objective(params):
    global iteration_count
    iteration_count += 1

    # 2. à¹à¸•à¸à¸•à¸±à¸§à¹à¸›à¸£ 6 à¸•à¸±à¸§ (à¸£à¸±à¸š batch_size à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²)
    n_layers, n_neurons, lr, dropout_rate, weight_decay, batch_size = params

    # 3. à¸ªà¸£à¹‰à¸²à¸‡ DataLoader à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™à¸à¹‰à¸­à¸™à¹† à¸•à¸²à¸¡à¸‚à¸™à¸²à¸” batch_size à¸—à¸µà¹ˆ GBRT à¸ªà¸¸à¹ˆà¸¡à¸¡à¸²
    train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True)

    model = FlexibleMLP(n_layers=n_layers, n_neurons=n_neurons, dropout_rate=dropout_rate)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.MSELoss()

    model.train()
    for epoch in range(20):
        # ==========================================
        # 4. à¸§à¸™à¸¥à¸¹à¸›à¸¢à¹ˆà¸­à¸¢à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¸¥à¸° Batch à¸¡à¸²à¹€à¸—à¸£à¸™
        # ==========================================
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

    model.eval()
    with torch.no_grad():
        preds = model(X_val_t).numpy()
        rmse = np.sqrt(mean_squared_error(y_val, preds))

    print(
        f"â³ à¸£à¸­à¸š {iteration_count}: layers={n_layers}, neurons={n_neurons}, lr={lr:.5f}, drop={dropout_rate:.2f}, wd={weight_decay:.5f}, batch={batch_size} -> RMSE: {rmse:.4f}")
    return rmse


class TargetScoreStopper(EarlyStopper):
    def __init__(self, target_score):
        self.target_score = target_score

    def _criterion(self, result):
        if result.fun <= self.target_score:
            print(
                f"\nðŸŽ‰ à¸«à¸¢à¸¸à¸”à¸à¸²à¸£à¸ˆà¸¹à¸™à¸à¹ˆà¸­à¸™à¸à¸³à¸«à¸™à¸”! à¸žà¸šà¸„à¹ˆà¸² RMSE ({result.fun:.4f}) à¸‹à¸¶à¹ˆà¸‡à¸œà¹ˆà¸²à¸™à¹€à¸à¸“à¸‘à¹Œà¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢ ({self.target_score}) à¹à¸¥à¹‰à¸§!")
            return True
        return False


# ==========================================
# 5. à¸à¸³à¸«à¸™à¸”à¸‚à¸­à¸šà¹€à¸‚à¸•à¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸² (à¹€à¸žà¸´à¹ˆà¸¡ Batch Size à¹à¸šà¸šà¹€à¸¥à¸‚à¸à¸²à¸™ 2)
# ==========================================
search_space = [
    Integer(1, 5, name='n_layers'),
    Integer(32, 256, name='n_neurons'),
    Real(1e-4, 1e-2, prior='log-uniform', name='lr'),
    Real(0.0, 0.5, name='dropout_rate'),
    Real(1e-5, 1e-3, prior='log-uniform', name='weight_decay'),
    Categorical([32, 64, 128, 256, 512], name='batch_size')  # à¹ƒà¸Šà¹‰ Categorical à¸šà¸±à¸‡à¸„à¸±à¸šà¹ƒà¸«à¹‰à¸ªà¸¸à¹ˆà¸¡à¹€à¸‰à¸žà¸²à¸°à¹€à¸¥à¸‚à¸à¸¥à¸¸à¹ˆà¸¡à¸™à¸µà¹‰
]

stopper = TargetScoreStopper(target_score=TARGET_RMSE)

print(f"à¸à¸³à¸¥à¸±à¸‡à¹€à¸£à¸´à¹ˆà¸¡à¹ƒà¸«à¹‰ GBRT à¸ˆà¸¹à¸™ 6 à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ... (à¸£à¸­à¸šà¸ªà¸¹à¸‡à¸ªà¸¸à¸”: {MAX_CALLS}, à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢ RMSE: <= {TARGET_RMSE})")
result = forest_minimize(objective, search_space, n_calls=MAX_CALLS, callback=[stopper], random_state=42)

print("\n=== à¸œà¸¥à¸à¸²à¸£à¸ˆà¸¹à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ===")
print(f"à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” (Layers, Neurons, LR, Dropout, WeightDecay, BatchSize): {result.x}")
print(f"à¸„à¹ˆà¸² RMSE à¸—à¸µà¹ˆà¸•à¹ˆà¸³à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸—à¸µà¹ˆà¸—à¸³à¹„à¸”à¹‰: {result.fun:.4f}")