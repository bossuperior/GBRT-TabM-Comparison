import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from pathlib import Path
import json
import random

import tabm  # ‡∏≠‡∏¥‡∏°‡∏û‡∏≠‡∏£‡πå‡∏ï‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏Ç‡∏≠‡∏á Yandex

# ==========================================
# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Data
# ==========================================
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data" / "california"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è [Train Status] ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå‡πÄ‡∏ó‡∏£‡∏ô TabM ‡∏î‡πâ‡∏ß‡∏¢: {device}")

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
X_train = torch.tensor(np.load(DATA_DIR / "X_num_train.npy")).float()
y_train = torch.tensor(np.load(DATA_DIR / "Y_train.npy")).float().view(-1, 1)

X_val = torch.tensor(np.load(DATA_DIR / "X_num_val.npy")).float()
y_val = torch.tensor(np.load(DATA_DIR / "Y_val.npy")).float().view(-1, 1)

X_test = torch.tensor(np.load(DATA_DIR / "X_num_test.npy")).float()
y_test_np = np.load(DATA_DIR / "Y_test.npy")

# ==========================================
# 2. üåü ‡πÇ‡∏´‡∏•‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON üåü
# ==========================================
json_path = BASE_DIR / "TabM_R2" / "tabm_best_params.json"

try:
    with open(json_path, 'r') as f:
        best_params = json.load(f)
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {best_params}")
except FileNotFoundError:
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {json_path} (‡∏£‡∏±‡∏ô tabm_r2_optuna_tuner.py ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á?)")
    print("‚ö†Ô∏è ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß...")
    best_params = {
        "n_blocks": 3, "d_block": 256, "lr": 0.001,
        "weight_decay": 1e-4, "dropout": 0.1, "batch_size": 256
    }

# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏™‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£
BEST_N_BLOCKS = best_params["n_blocks"]
BEST_D_BLOCK = best_params["d_block"]
BEST_LR = best_params["lr"]
BEST_WEIGHT_DECAY = best_params["weight_decay"]
BEST_DROPOUT = best_params["dropout"]
BEST_BATCH_SIZE = best_params["batch_size"]

K_ENSEMBLE = 32  # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á TabM

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏î‡πâ‡∏ß‡∏¢ Batch Size ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=BEST_BATCH_SIZE, shuffle=True)

# ==========================================
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• TabM ‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# ==========================================
model = nn.Sequential(
    tabm.EnsembleView(k=K_ENSEMBLE),
    tabm.MLPBackboneBatchEnsemble(
        d_in=8,
        n_blocks=BEST_N_BLOCKS,
        d_block=BEST_D_BLOCK,
        dropout=BEST_DROPOUT,
        k=K_ENSEMBLE,
        tabm_init=True,
        scaling_init='normal',
        start_scaling_init_chunks=None,
    ),
    tabm.LinearEnsemble(BEST_D_BLOCK, 1, k=K_ENSEMBLE)
).to(device)

optimizer = optim.Adam(model.parameters(), lr=BEST_LR, weight_decay=BEST_WEIGHT_DECAY)
criterion = nn.MSELoss()

# ==========================================
# 4. Training Loop ‡∏û‡∏£‡πâ‡∏≠‡∏° Early Stopping
# ==========================================
MAX_EPOCHS = 200
PATIENCE = 20
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = BASE_DIR / "TabM_R2" / "best_tabm_model.pt"

print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô TabM...")

for epoch in range(MAX_EPOCHS):
    model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)

        # ‡∏Ç‡∏¢‡∏≤‡∏¢ Label ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Ensemble
        y_expanded = batch_y.unsqueeze(1).expand(-1, K_ENSEMBLE, -1)
        loss = criterion(outputs, y_expanded)
        loss.backward()
        optimizer.step()

    # --- Validation Phase ---
    model.eval()
    with torch.no_grad():
        X_val_dev, y_val_dev = X_val.to(device), y_val.to(device)
        val_outputs = model(X_val_dev)

        # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á 32 ‡∏£‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î‡∏ú‡∏•
        final_val_pred = val_outputs.mean(dim=1)
        val_loss = criterion(final_val_pred, y_val_dev).item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1:3d}/{MAX_EPOCHS}] | Val MSE: {val_loss:.4f}")

    # Early Stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), best_model_path)
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= PATIENCE:
        print(f"\nüõë ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏ö {epoch + 1}! ‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤...")
        break

# ==========================================
# 5. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏™‡∏ô‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á (Test Set)
# ==========================================
model.load_state_dict(torch.load(best_model_path))
model.eval()

with torch.no_grad():
    X_test_dev = X_test.to(device)
    test_outputs = model(X_test_dev)

    # ‡∏£‡∏ß‡∏°‡∏û‡∏•‡∏±‡∏á 32 ‡∏£‡πà‡∏≤‡∏á ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡∏≠‡∏ô‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏ô‡∏≠‡∏•
    final_test_pred = test_outputs.mean(dim=1).cpu().numpy()

    final_test_rmse = np.sqrt(mean_squared_error(y_test_np, final_test_pred))
    final_test_r2 = r2_score(y_test_np, final_test_pred)

print("\n=========================================")
print(f"üèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ù‡∏±‡πà‡∏á TabM (‡∏ö‡∏ô TEST SET")
print(f"RMSE: {final_test_rmse:.4f}")
print(f"R¬≤ Score: {final_test_r2:.4f}")
print("=========================================")

# ‡πÄ‡∏ã‡∏ü‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ main.py ‡∏ó‡∏≥‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
results_file = BASE_DIR / "TabM" / "tabm_final_results.json"
final_results = {
    "model_name": "TabM (Optuna Tuned)",
    "test_rmse": float(final_test_rmse),
    "test_r2": float(final_test_r2)
}
with open(results_file, "w", encoding="utf-8") as f:
    json.dump(final_results, f, indent=4)

print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á‡πÉ‡∏ô {results_file} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")